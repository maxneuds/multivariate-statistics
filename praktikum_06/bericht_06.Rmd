---
output: pdf_document
header-includes: 
- \usepackage[utf8]{inputenc}
- \usepackage[T1]{fontenc}
- \usepackage[ngerman]{babel}
- \usepackage{amsmath,amssymb,amsthm}
- \usepackage{dsfont}
- \usepackage{listings}
- \usepackage{floatrow}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- '\fancyhead[C,C]{Gruppe 11: Robin Baudisch, Merlin Kopfmann, Maximilian Neudert}'
- \fancyhead[L]{}
- \fancyhead[R]{}
- \fancyfoot[C,C]{\thepage}
- \renewcommand{\footrulewidth}{0.4pt}
- '\newcommand{\cov}[1]{\operatorname{Cov}\left( #1 \right)}'
- '\newcommand{\cor}[1]{\operatorname{Cor}\left( #1 \right)}'
- '\newcommand{\V}[1]{\mathbf{\operatorname{V}}\left( #1 \right)}'
- '\newcommand{\E}[1]{\mathbf{\operatorname{E}}\left( #1 \right)}'
---

<style type="text/css">
body{
  font-size: 12px;
}
h1 {
  font-size: 18px;
}
h1 {
  font-size: 14px;
}
h1 {
  font-size: 12px;
}
</style>

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,     # Keep compiling upon error
                      collapse=TRUE,  # collapse by default
                      echo=TRUE,      # echo code by default
                      comment = "#>", # change comment character
                      warning=TRUE,   # show R warnings
                      message=FALSE,  # show R messages
                      out.width = "100%",
                      out.height = "100%",
                      fig.width = 10)
```

<!---** Hochschule Darmstadt | Studiengang Data Science | Sommersemester 2019 **--->

```{r}
set.seed(42)
```

# Arbeitsblatt 6

## Aufgabe 1

### a)
```{r}
load("prostate.Rdata")

hist(prostate$age)
hist(prostate$svi)
hist(prostate$gleason)
hist(prostate$pgg45)
hist(prostate$psa)
hist(prostate$cavol)


prostate$pgg45log <- log(prostate$pgg45)
prostate$pgg45log[is.infinite(prostate$pgg45log)] <- -exp(99)
prostate$cavollog <- log(prostate$cavol)
prostate$cavollog[is.infinite(prostate$cavollog)] <- -exp(99)

linreg_log <- lm(log(psa) ~ age+svi+log(gleason)+pgg45log+cavollog, data = prostate)
linreg <- lm(psa ~ age+svi+gleason+pgg45+cavol, data = prostate)
summary(linreg_log)
summary(linreg)
AIC(linreg_log, linreg)
```

Das Modell mit logarithmierten Kovariaten gleason, pgg45 und cavol und abhängiger Variable log(psa) ist deutlich besser als das Modell ohne Variablentransformation. Dies lässt sich zum einen am R^2-Wert ablesen, als auch über den AIC-Vergleich feststellen. 

### b)
```{r}
library("MASS")
library("leaps")

cat("Modellvergleich: StepAIC Backward & StepAIC Forward:")
cat("\n")
stepAIC(linreg_log, direction = "backward")
linreg_forward <- lm(log(psa) ~ 1, data = prostate)
stepAIC(linreg_forward, direction = "forward", scope=list(upper=linreg_log,lower=linreg_forward))

regfit.full <- regsubsets(log(psa) ~ age+svi+log(gleason)+pgg45log+cavollog, data = prostate)
reg.summary <- summary(regfit.full)

plot(reg.summary$rsq, type='l', xlab="Number of Variables", ylab="R-Squared")
plot(reg.summary$cp, type='l', xlab="Number of Variables", ylab="AIC")
plot(reg.summary$bic, type='l', xlab="Number of Variables", ylab="BIC")

cat("Bestes Modell laut regsubsets:")
cat("\n")
coef(regfit.full, 3)
```

Laut StepAIC ist das Modell "log(psa) ~ svi + pgg45log + cavollog" das Beste (Sowohl backward als auch forward).
variablenselektion durch Regsubsets ist nicht eindeutig. Laut BIC ist das Modell mit 2 Kovariaten das Beste, AIC und $R^2$ sprechen für das Modell mit 3 Kovariaten. Dieses ist das Gleiche, wie im StepAIC. Wir entscheiden uns deshalb für dieses Modell. 

### c)
```{r}
linreg_final <- lm(log(psa) ~ svi + pgg45log + cavollog, data=prostate)
plot(linreg)
plot(linreg_final)
```

\cleardoublepage

## Aufgabe 2

Wir können die Unkorreliertheit einerseits beweisen durch

\begin{align}
\cov{Y_i, Y_j}
& = \cov{(A^T X)_i, (A^T X)_j} \\
& = \cov{A^T X}_{ij} \\
& = \left( A^T \cov{X} A \right)_{ij} \\
& = \left( A^T \Sigma A \right)_{ij} \\
& = \Lambda_{ij} \\
& = \begin{cases}
1, & i = j,\\
0, & \text{sonst}.
\end{cases}
\end{align}

Alternativ zur besseren Vorstellung.
Sei $e_i$ der $i$-te Einheitsvektor, dann erhalten wir:

\begin{align}
\cov{Y_i, Y_j}
& = \cov{(A^T X)_i, (A^T X)_j} \\
& = \cov{e_i^T A^T X, e_j^T A^T X} \\
& = e_i^T \cov{A^T X, A^T X} e_j \\
& = e_i^T A^T \cov{X, X} A e_j \\
& = e_i^T A^T \Sigma A e_j \\
& = e_i^T \Lambda e_j \\
& = \Lambda_i e_j \\
& = \Lambda_{ij}
& = \begin{cases}
1, & i = j,\\
0, & \text{sonst}.
\end{cases}
\end{align}

\cleardoublepage

## Aufgabe 3

### a)

```{r}
library(ggplot2)
data = read.csv('framingham.csv', sep=';')
df = data.frame(DBP = data$DBP, SBP = data$SBP)
x1 = df$DBP
x2 = df$SBP
gg = ggplot(
  data = df,
  mapping = aes(
    x = DBP,
    y = SBP
  )
)
gg + geom_point()
cor = round(cor(x1, x2), 3)
```

Wir erhalten eine Korrelation von $`r cor`$. DBP und SBP sind also positiv korreliert.

### b)

```{r}
norm_vec = function(x) {
  return(x / sqrt(sum(x^2)))
}
a = norm_vec(c(3,5))
X = matrix(c(x1, x2), nrow = length(x1), ncol = 2)
Y = t(a) %*% t(X)
Y = t(Y)
y = c(Y)
ev = round(var(y) / (var(x1) + var(x2)), 3)
```

Wir erhalten eine erklärende Varianz von $`r ev`$.

### c)

```{r}
f = function(a1,a2) {
  a = norm_vec(c(a1,a2))
  X = matrix(c(x1, x2), nrow = length(x1), ncol = 2)
  Y = t(a) %*% t(X)
  Y = t(Y)
  y = c(Y)
  return(ev = round(var(y) / (var(x1) + var(x2)), 3))
}
f1 = function(a1) {
  a = norm_vec(c(a1,1))
  X = matrix(c(x1, x2), nrow = length(x1), ncol = 2)
  Y = t(a) %*% t(X)
  Y = t(Y)
  y = c(Y)
  return(ev = round(var(y) / (var(x1) + var(x2)), 3))
}
a1 = optimize(f1, c(0, 1), maximum = TRUE)$maximum
f2 = function(a2) {
  a = norm_vec(c(a1,a2))
  X = matrix(c(x1, x2), nrow = length(x1), ncol = 2)
  Y = t(a) %*% t(X)
  Y = t(Y)
  y = c(Y)
  return(ev = round(var(y) / (var(x1) + var(x2)), 3))
}
a2 = optimize(f2, c(0, 1), maximum = TRUE)$maximum
sol = f(a1,a2)
a = norm_vec(c(a1,a2))
```

Wir optimieren erst $a_1$ mit $(a_1, 1) und anschließend $a_2$ und erhalten:
$a_1 = `r round(a[1], 3)`$
$a_2 = `r round(a[2], 3)`$
mit erklärender Varianz von $`r sol`$.

### d)

```{r}
l = loadings(princomp(X))[,1]
sol2 = f(l[1], l[2])
```

Mit der in R eingebauten Funktion erhalten wir
$a_1 = `r round(l[1], 3)`$
$a_2 = `r round(l[2], 3)`$
mit erklärender Varianz von $`r sol2`$.

Verglichen mit unserem Ergebnis erhalten wir ein etwas unterschiedliches $a$, aber mit gleichen maximalen erklärenden Erwartungswert.






